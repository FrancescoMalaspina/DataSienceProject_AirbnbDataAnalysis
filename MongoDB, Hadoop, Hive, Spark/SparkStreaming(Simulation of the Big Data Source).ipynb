{"cells":[{"cell_type":"markdown","id":"21ee9b07","metadata":{"id":"21ee9b07"},"source":["# Big Data source - Airbnb website\n","We hypothesized that our big data source is the aribnb website, which is constantly feeding us new reviews, new listings, new calendar (booking plan for the subsequent year) data. This means that we can model it as a streaming source\n","\n","We can **control** this streaming source with the SparkStreaming library, which we connected to a mongoDB dataset with the mongoDB - spark connector (presented in the apposite notebook).\n","\n","This notebook contains a simulation of the streaming source. We obtained it using a `SparkSession.readStream` object. We streamed the csv files (listings.csv, reviews.csv, ...) in a directory, specifying the schema with a `StructType`. In the real world application we would receive the streaming from Kafka or directly from a TCP socket, but SparkStreaming allows to handle it in the exact same way."]},{"cell_type":"code","source":["import findspark\n","findspark.init()\n","import pyspark\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","from pyspark.sql import SQLContext\n","from pyspark.sql import Row\n","\n","# specify the app name\n","appName = \"streaming_pyspark\"\n","\n","# I enabled 2 cores for the virtual machine\n","# 2 is the number of enabled RDD or Dataframe partitions\n","master = \"local[2]\" "],"metadata":{"id":"v_BxOGPWOV33"},"id":"v_BxOGPWOV33","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After importing the necessary modules and initialized spark (with `findspark.init()`) we created a SparkSession, connected to mongoDB."],"metadata":{"id":"igydEelGOdik"},"id":"igydEelGOdik"},{"cell_type":"code","source":["input_uri = \"mongodb+srv://analytics:analytics-password@mflix.ryqp8qp.mongodb.net/?retryWrites=true&w=majority\"\n","ouput_uri = \"mongodb+srv://analytics:analytics-password@mflix.ryqp8qp.mongodb.net/?retryWrites=true&w=majority\"\n","\n","\n","myspark = SparkSession\\\n","    .builder\\\n","    .appName(\"MyApp\")\\\n","    .config(\"spark.mongodb.input.uri\", input_uri)\\\n","    .config(\"spark.mongodb.output.uri\", ouput_uri)\\\n","    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:2.4.2')\\\n","    .getOrCreate()"],"metadata":{"id":"0IfyDupzOzlv"},"id":"0IfyDupzOzlv","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3c46d6e3","metadata":{"id":"3c46d6e3"},"source":["# Simulation of the streming source\n","This is were the real simulation begins, we defined the schema, read the streaming from the csv files and checked is the resulting object `isStreaming()`.\n","\n","We also printed part of the streming on the console to check if it was reading the files correctly. Then we wrote the stream on MongoDB."]},{"cell_type":"code","execution_count":null,"id":"6e376f4b","metadata":{"id":"6e376f4b","outputId":"a47f287b-72fb-4a24-dc77-07da8c58344a"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["from pyspark.sql.types import FloatType, StructField, StructType, StringType, DateType, IntegerType\n","# specify the schema (we are streaming the csv file, not loading it,\n","#so we can't go over it all to infer the schema)\n","reviewsSchema = StructType([\n","    StructField(\"listing_id\",IntegerType(), True),\n","    StructField(\"id\",IntegerType(), True),\n","    StructField(\"date\",DateType(), True),\n","    StructField(\"reviewer_id\",IntegerType(), True),\n","    StructField(\"reviewer_name\",StringType(), True),\n","    StructField(\"comment\",StringType(), True)\n","])\n","# read the csv file reviews, by streaming it (readStream method instead of just read)\n","reviews_streamed = spark.readStream.schema(reviewsSchema).option(\"maxfilesperTrigger\",1).csv(\"./Florence/Streaming_Simulation/\", header = True)\n","\n","#check if data are streaming\n","print(reviews_streamed.isStreaming)"]},{"cell_type":"code","execution_count":null,"id":"2f219f0b","metadata":{"id":"2f219f0b","outputId":"40e7a979-ce2b-48f1-f7d1-0ae19df92026"},"outputs":[{"name":"stderr","output_type":"stream","text":["22/09/17 17:20:23 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e35afd63-5b8d-4baa-9197-e19b5b8186d1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n","22/09/17 17:20:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n","22/09/17 17:20:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n"," Header: listing_id, id, date, reviewer_id, reviewer_name, comments\n"," Schema: listing_id, _id, date, reviewer_id, reviewer_name, comment\n","Expected: _id but found: id\n","CSV file: file:///home/dsbda/DS_project/Florence/Streaming_Simulation/reviews.csv\n"]}],"source":["# write stream on the console, to see if it works\n","reviews_streamed.writeStream.format(\"console\").outputMode(\"append\").start()  # IT DOES! :)\n","\n","# simple group by\n","dfc = reviews_streamed.groupby(\"date\").count()\n","#dfc.writeStream.outputMode(\"complete\").format(\"console\").start().\n"]},{"cell_type":"code","source":["reviews_streamed.writeStream.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",".option(\"uri\", input_uri)\\\n",".option(\"database\", \"Florence\")\\\n",".option(\"collection\", \"reviews\")\\\n",".outputMode(\"append\").start()"],"metadata":{"id":"OhmrE6tWRX3Z"},"id":"OhmrE6tWRX3Z","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f4316618","metadata":{"id":"f4316618"},"source":["# MongoDB queries and long term storage on hive databases\n","Now we can use mongoDB to perfom fast queries in real time on the newly imported data.\n","\n","Also we can periodically (e.g. every month) transfer all new data to HDFS as hive tables, in order to build a datawarehouse for long term storage and analysis on the entire dataset.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}